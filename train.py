import os
import zipfile
import json
import requests
import subprocess
from ultralytics import YOLO
import multiprocessing
import torch
import glob
import random 
import shutil 
import yaml # â­ YAML íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

def download_kaggle_dataset(kaggle_json_path, dataset_name, download_path):
    """
    Kaggle APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
    """
    os.environ["KAGGLE_CONFIG_DIR"] = os.path.dirname(kaggle_json_path)
    
    cmd = [
        "kaggle", "datasets", "download",
        "-d", dataset_name,
        "-p", download_path,
        "--force"
    ]
    print(f"[INFO] Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘: {dataset_name}")
    subprocess.run(cmd, check=True)
    
    # ë‹¤ìš´ë¡œë“œëœ ZIP íŒŒì¼ ìë™ íƒì§€
    zip_files = glob.glob(os.path.join(download_path, "*.zip"))
    if not zip_files:
        raise FileNotFoundError("[ERROR] ë‹¤ìš´ë¡œë“œëœ ZIP íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    zip_path = zip_files[0]
    
    # ì••ì¶• í•´ì œ
    print(f"[INFO] ì••ì¶• í•´ì œ ì¤‘: {zip_path}")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(download_path)
    print("[INFO] ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ ì™„ë£Œ.")
    return zip_path
def load_config():
    """
    setup/config.yamlì—ì„œ Kaggle URL ì½ê¸°
    """
    config_path = os.path.join("setup", "config.yaml")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"[ERROR] config.yaml íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {config_path}")
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    dataset_name = config.get("kaggle", {}).get("dataset_url")
    if not dataset_name:
        raise ValueError("[ERROR] config.yamlì— kaggle.dataset_urlì´ ì§€ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return dataset_name
def main():
    # ===============================
    # 1ï¸âƒ£ ê²½ë¡œ ì„¤ì •
    # ===============================
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë” (í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤í–‰ë˜ëŠ” ìœ„ì¹˜)
    project_root = os.path.dirname(os.path.abspath(__file__)) 
    
    # ì„¤ì • íŒŒì¼ ê²½ë¡œ
    setup_dir = os.path.join(project_root, "setup")
    os.makedirs(setup_dir, exist_ok=True)

    # ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ
    dataset_dir = os.path.join(project_root, "datasets", "css-data")
    os.makedirs(dataset_dir, exist_ok=True)

    # Kaggle ì¸ì¦ íŒŒì¼ ê²½ë¡œ í™•ì¸
    kaggle_json_path = os.path.join(setup_dir, "kaggle.json")
    if not os.path.exists(kaggle_json_path):
        raise FileNotFoundError(f"[ERROR] Kaggle ì¸ì¦ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (setup/kaggle.json)")

    # ===============================
    # 2ï¸âƒ£ Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ / ìµœì‹  ê°±ì‹ 
    # ===============================
    dataset_name = load_config()
    
    # ZIP íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    zip_files = glob.glob(os.path.join(dataset_dir, "*.zip"))
    if zip_files:
        zip_path = zip_files[0]
        print(f"[INFO] ê¸°ì¡´ ZIP íŒŒì¼ ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {zip_path}")
    else:
        # ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ í•¨ìˆ˜ í˜¸ì¶œ
        zip_path = download_kaggle_dataset(kaggle_json_path, dataset_name, dataset_dir)

    # ===============================
    # 3ï¸âƒ£ ì••ì¶• í•´ì œ (ìµœì‹  ë°ì´í„° ìœ ì§€)
    # ===============================
    # 'train' í´ë”ê°€ ì—†ìœ¼ë©´ ì••ì¶• í•´ì œ ì§„í–‰
    if not os.path.exists(os.path.join(dataset_dir, "train")):
        print("[INFO] ë°ì´í„°ì…‹ ì••ì¶• í•´ì œ ì¤‘...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(dataset_dir)
        print("[INFO] ì••ì¶• í•´ì œ ì™„ë£Œ.")
    else:
        print("[INFO] ì••ì¶• í•´ì œëœ ë°ì´í„°ì…‹ í´ë” ì¡´ì¬, ìµœì‹  ë°ì´í„° ìœ ì§€.")

    # ===============================
    # 3.5ï¸âƒ£ ì‹¤ì œ ë°ì´í„°ì…‹ ë£¨íŠ¸ í´ë” ë™ì  ì°¾ê¸° ë° í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ (ìˆ˜ì •ëœ ë¶€ë¶„)
    # ===============================
    data_root = dataset_dir
    original_data_yaml_path = None
    
    # 1. train í´ë”ë¥¼ í¬í•¨í•˜ëŠ” ì‹¤ì œ ë°ì´í„° ë£¨íŠ¸ í´ë” ì°¾ê¸°
    if not os.path.exists(os.path.join(data_root, "train")):
        subdirs = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]
        found = False
        for subdir in subdirs:
            if os.path.exists(os.path.join(dataset_dir, subdir, "train")):
                data_root = os.path.join(dataset_dir, subdir)
                print(f"[INFO] ì‹¤ì œ ë°ì´í„° ë£¨íŠ¸ í´ë” ë°œê²¬: {data_root}")
                found = True
                break
        
        if not found:
            raise FileNotFoundError(f"[CRITICAL ERROR] '{dataset_dir}' ë‚´ë¶€ì—ì„œ 'train' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤.")
    
    # 2. ë°ì´í„°ì…‹ ë‚´ë¶€ì˜ data.yamlì—ì„œ í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ
    original_data_yaml_path = glob.glob(os.path.join(data_root, "*.yaml"))
    
    if not original_data_yaml_path:
        # data.yaml íŒŒì¼ì´ ì—†ìœ¼ë©´ í•˜ë“œì½”ë”©ëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ
        print("[WARN] ë°ì´í„°ì…‹ ë£¨íŠ¸ì—ì„œ data.yaml íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ë“œì½”ë”©ëœ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        class_names = ['Hardhat','Mask','NO-Hardhat','NO-Mask','NO-Safety Vest','Person','Safety Cone','Safety Vest','machinery','vehicle']
        num_classes = 10
    else:
        original_data_yaml_path = original_data_yaml_path[0]
        try:
            with open(original_data_yaml_path, 'r', encoding='utf-8') as f:
                original_yaml = yaml.safe_load(f)
            class_names = original_yaml.get('names', [])
            num_classes = original_yaml.get('nc', len(class_names))
            print(f"[INFO] ê¸°ì¡´ data.yamlì—ì„œ í´ë˜ìŠ¤ ì •ë³´ ({num_classes}ê°œ)ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"[ERROR] ê¸°ì¡´ data.yaml íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}. í•˜ë“œì½”ë”©ëœ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            class_names = ['Person','vehicle']
            num_classes = 10


    # ===============================
    # 4ï¸âƒ£ val í´ë” ìë™ ìƒì„± (ê²½ë¡œ ì„¤ì • ì‹œ data_root ì‚¬ìš©)
    # ===============================
    train_path = os.path.join(data_root, "train/images") 
    train_labels_path = os.path.join(data_root, "train/labels") 
    val_path = os.path.join(data_root, "valid/images") 
    val_labels_path = os.path.join(data_root, "valid/labels") 
    test_path = os.path.join(data_root, "test/images") 
    
    # ğŸš¨ í•„ìˆ˜: í•™ìŠµìš© ë¼ë²¨ í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(train_labels_path):
        raise FileNotFoundError(f"[CRITICAL ERROR] í•™ìŠµ ë¼ë²¨ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_labels_path}. ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. (train/imagesê°€ ìˆë‹¤ë©´, train/labelsë„ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.)")

    # í•™ìŠµìš© ì´ë¯¸ì§€ ëª©ë¡ì„ ë¨¼ì € ê°€ì ¸ì˜µë‹ˆë‹¤.
    train_images = glob.glob(os.path.join(train_path, "*"))

    if not os.path.exists(val_path):
        if not train_images:
            # í•™ìŠµìš© ì´ë¯¸ì§€ê°€ ë¹„ì–´ìˆì„ ê²½ìš° (ì´ì „ random.sample ì˜¤ë¥˜ ë°©ì§€)
            print(f"[ERROR] '{train_path}'ì—ì„œ í•™ìŠµìš© ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ main í•¨ìˆ˜ ì¢…ë£Œ
        else:
            print("[INFO] valid/images í´ë”ê°€ ì—†ì–´ train ë°ì´í„°ë¥¼ ì¼ë¶€ ë³µì‚¬í•©ë‹ˆë‹¤.")
            os.makedirs(val_path, exist_ok=True)
            os.makedirs(val_labels_path, exist_ok=True) 
            
            # ìƒ˜í”Œë§ ìˆ˜ ê³„ì‚°: ì „ì²´ì˜ 10%, ìµœì†Œ 1ê°œ
            val_count = max(1, len(train_images)//10) 
            
            # random.sample í˜¸ì¶œ (ì´ì œ train_imagesê°€ ë¹„ì–´ìˆì§€ ì•ŠìŒì´ ë³´ì¥ë¨)
            val_sample = random.sample(train_images, val_count)  # 10% ìƒ˜í”Œ
            
            for img_file in val_sample:
                # 1. ì´ë¯¸ì§€ ë³µì‚¬
                shutil.copy(img_file, val_path)
                
                # 2. ë¼ë²¨ íŒŒì¼ ì°¾ê¸° ë° ë³µì‚¬
                base_name = os.path.splitext(os.path.basename(img_file))[0]
                src_label_file = os.path.join(train_labels_path, base_name + ".txt") 
                dst_label_file = os.path.join(val_labels_path, base_name + ".txt")

                if os.path.exists(src_label_file):
                    shutil.copy(src_label_file, dst_label_file)
                else:
                    print(f"[WARN] í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ ë¼ë²¨ íŒŒì¼ ëˆ„ë½: {src_label_file}. ìœ íš¨ì„± ê²€ì‚¬ì—ì„œ ì´ ì´ë¯¸ì§€ëŠ” ì œì™¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            print(f"[INFO] ìœ íš¨ì„± ê²€ì‚¬ ë°ì´í„° {len(val_sample)}ê°œ ìƒì„± ì™„ë£Œ (ì´ë¯¸ì§€ ë° ë¼ë²¨).")
    else:
        print("[INFO] ì••ì¶• í•´ì œëœ ë°ì´í„°ì…‹ í´ë” ì¡´ì¬, ìµœì‹  ë°ì´í„° ìœ ì§€.")


    # ===============================
    # 5ï¸âƒ£ data.yaml ìƒì„± (ë™ì  í´ë˜ìŠ¤ ì •ë³´ ë°˜ì˜)
    # ===============================
    # í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    names_str = str(class_names).replace("'", "")
    
    yaml_content = f"""
train: {train_path.replace(os.sep, '/')}
val:   {val_path.replace(os.sep, '/')}
test:  {test_path.replace(os.sep, '/')}
nc: {num_classes}
names: {names_str}
"""
    yaml_path = os.path.join(setup_dir, "data.yaml")
    with open(yaml_path, "w", encoding="utf-8") as f:
        f.write(yaml_content)
    print(f"[INFO] data.yaml ìƒì„± ì™„ë£Œ: {yaml_path}")

    # ===============================
    # 6ï¸âƒ£ YOLOv8 ëª¨ë¸ í•™ìŠµ
    # ===============================
    # (ì´ë¯¸ì§€/ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš° main í•¨ìˆ˜ê°€ ì´ë¯¸ ì¢…ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[INFO] í˜„ì¬ ì‚¬ìš© ì¥ì¹˜: {device}")

    model = YOLO("yolov8s.pt")

    results = model.train(
        data=yaml_path,
        epochs=50,
        imgsz=640,
        batch=16,
        device=device,
        name=f"construction_yolov8_{device}",
        save_period=10,
        verbose=True,
    )

    print("[INFO] í•™ìŠµ ì™„ë£Œ")
    print(f"[INFO] ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: runs/train/construction_yolov8_{device}")

# ===============================
# âœ… Windows ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì „ ì‹¤í–‰
# ===============================
if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()
